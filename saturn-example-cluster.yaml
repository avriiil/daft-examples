# A unique identifier for the head node and workers of this cluster.
# EC2 instances will open as ray-${cluster_name}-head or
# ray-${cluster_name}-worker
cluster_name: demo

# The maximum number of workers nodes to launch in addition to the head node.
# This takes precedence over min_workers which defaults to 0.
max_workers: 5

# Cloud-provider-specific configuration.
provider:
  type: aws
  region: eu-west-2
  use_internal_ips: true
  # You can define other regions to open up worker nodes here.
  # availability_zone: us-east-1,us-west-2a,us-west-2b

  cache_stopped_nodes: False

# Define all the different node schemas in your cluster here.
# For our demo, we'll define two nodes (can be arbitrary names)
# a head node and a worker node.
available_node_types:
  head_node:
    node_config:
      InstanceType: t2.xlarge

      # Choose the instance image you want.
      # You can find these IDs when you attempt to
      # make a new AWS instance on the AWS Console
      ImageId: ami-01ec84b284795cbc7

      # Define disk space of instance
      BlockDeviceMappings:
        - DeviceName: /dev/sda1
          Ebs:
            VolumeSize: 150

  worker_nodes:
    node_config:
      InstanceType: t2.xlarge
      ImageId: ami-01ec84b284795cbc7
      BlockDeviceMappings:
        - DeviceName: /dev/sda1
          Ebs:
            VolumeSize: 150

# Define the name of the head node from above here.
head_node_type: head_node

# How Ray will authenticate with newly launched nodes.
auth:
  ssh_user: ubuntu
  # By default Ray creates a new private keypair,
  # but you can also use your own.
  # If you do so, make sure to also set "KeyName" in the head and worker node
  # configurations below.
  # ssh_private_key: /path/to/your/key.pem

# These commands are run every time any new node is created.
setup_commands:
  # Two first lines are a workaround for ssh timing out
  - sleep 4
  - sudo apt update
  - sudo apt install -y python3-pip python-is-python3
  - pip install ray[default] boto3 torch
